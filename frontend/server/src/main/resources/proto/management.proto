syntax = "proto3";

package org.pytorch.serve.grpc.management;

option java_multiple_files = true;

message DescribeModelRequest {
    // Name of model to describe.
    string model_name = 1;
    string model_version = 2;
}

message DescribeModelResponse {
    // Name of model to describe.
    string describe_model_response = 1;
    int32 status_code = 2;
    string info = 3;
}

message ListModelsRequest {
    // Use this parameter to specify the maximum number of items to return. When this value is present, TorchServe does not return more than the specified number of items, but it might return fewer. This value is optional. If you include a value, it must be between 1 and 1000, inclusive. If you do not include a value, it defaults to 100.
    int32 limit = 1;

    // The token to retrieve the next set of results. TorchServe provides the token when the response from a previous call has more results than the maximum page size.
    string next_page_token = 2;
}

message ListModelsResponse {
    // Use this parameter to specify the maximum number of items to return. When this value is present, TorchServe does not return more than the specified number of items, but it might return fewer. This value is optional. If you include a value, it must be between 1 and 1000, inclusive. If you do not include a value, it defaults to 100.
    int32 limit = 1;

    // The token to retrieve the next set of results. TorchServe provides the token when the response from a previous call has more results than the maximum page size.
    string next_page_token = 2;

    int32 status_code = 3;

    string info = 4;
}

message RegisterModelRequest {
    enum RegisterModelRequestRuntime {
        REGISTER_MODEL_REQUEST_RUNTIME_PYTHON = 0;
        REGISTER_MODEL_REQUEST_RUNTIME_PYTHON2 = 1;
        REGISTER_MODEL_REQUEST_RUNTIME_PYTHON3 = 2;
    }

    // Inference batch size, default: 1.
    int32 batch_size = 1;

    // Inference handler entry-point. This value will override handler in MANIFEST.json if present.
    string handler = 2;

    // Number of initial workers, default: 0.
    int32 initial_workers = 3;

    // Maximum delay for batch aggregation, default: 100.
    int32 max_batch_delay = 4;

    // Name of model. This value will override modelName in MANIFEST.json if present.
    string model_name = 5;

    // Maximum time, in seconds, the TorchServe waits for a response from the model inference code, default: 120.
    int32 response_timeout = 6;

    // Runtime for the model custom service code. This value will override runtime in MANIFEST.json if present.
    RegisterModelRequestRuntime runtime = 7;

    // Decides whether creation of worker synchronous or not, default: false.
    bool synchronous = 8;

    // Model archive download url, support local file or HTTP(s) protocol. For S3, consider use pre-signed url.
    string url = 9;
}

message RegisterModelResponse {
    // Use this parameter to specify the maximum number of items to return. When this value is present, TorchServe does not return more than the specified number of items, but it might return fewer. This value is optional. If you include a value, it must be between 1 and 1000, inclusive. If you do not include a value, it defaults to 100.
    string register_response = 1;
    int32 status_code = 2;
    string info = 3;
}

message ScaleWorkerRequest {

    // Name of model to scale workers.
    string model_name = 1;

    // Model version.
    int32 model_version = 2;

    // Maximum number of worker processes.
    int32 max_worker = 3;

    // Minimum number of worker processes.
    int32 min_worker = 4;

    // Number of GPU worker processes to create.
    int32 number_gpu = 5;

    // Decides whether the call is synchronous or not, default: false.
    bool synchronous = 6;

    // Waiting up to the specified wait time if necessary for a worker to complete all pending requests. Use 0 to terminate backend worker process immediately. Use -1 for wait infinitely.
    int32 timeout = 7;
}

message ScaleWorkerResponse {
    string scale_worker_response = 1;
    int32 status_code = 2;
    string info = 3;
}

message SetDefaultRequest {
    // Name of model whose default version needs to be updated.
    string model_name = 1;

    // Version of model to be set as default version for the model
    string model_version = 2;
}

message SetDefaultResponse {
    string set_default_response = 1;
    int32 status_code = 2;
    string info = 3;
}

message UnregisterModelRequest {
    // Name of model to unregister.
    string model_name = 1;

    // Name of model to unregister.
    string model_version = 2;

    // Decides whether the call is synchronous or not, default: false.
    bool synchronous = 3;

    // Waiting up to the specified wait time if necessary for a worker to complete all pending requests. Use 0 to terminate backend worker process immediately. Use -1 for wait infinitely.
    int32 timeout = 4;
}

message UnregisterModelResponse {
    string unregister_model_response = 1;
    int32 status_code = 2;
    string info = 3;
}

service ManagementAPIsService {
    // Provides detailed information about the default version of a model.
    rpc DescribeModel(DescribeModelRequest) returns (DescribeModelResponse) {}

    // List registered models in TorchServe.
    rpc ListModels(ListModelsRequest) returns (ListModelsResponse) {}

    // Register a new model in TorchServe.
    rpc RegisterModel(RegisterModelRequest) returns (RegisterModelResponse) {}

    // Configure number of workers for a default version of a model.This is a asynchronous call by default. Caller need to call describeModel to check if the model workers has been changed.
    rpc ScaleWorker(ScaleWorkerRequest) returns (ScaleWorkerResponse) {}

    // Set default version of a model
    rpc SetDefault(SetDefaultRequest) returns (SetDefaultResponse) {}

    // Unregister the default version of a model from TorchServe if it is the only version available.This is a asynchronous call by default. Caller can call listModels to confirm model is unregistered
    rpc UnregisterModel(UnregisterModelRequest) returns (UnregisterModelResponse) {}
}